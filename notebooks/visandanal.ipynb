{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Import our custom modules (assume they're in the same directory)\n",
    "from notebook_1_data_prep_feature_extraction import MultiScaleFeatureExtractor\n",
    "from notebook_2_hierarchical_transformer_implementation import HierarchicalTransformer\n",
    "from notebook_3_training_and_evaluation import IntegratedModel\n",
    "\n",
    "# Constants\n",
    "NUM_CLASSES = 10\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Load a pre-trained model (assume we've trained and saved it)\n",
    "def load_model():\n",
    "    model = IntegratedModel().to(DEVICE)\n",
    "    model.load_state_dict(torch.load('hierarchical_transformer_model.pth'))\n",
    "    model.eval()\n",
    "    return model\n",
    "\n",
    "# Prepare a single image for input\n",
    "def prepare_image(img_path):\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "    img = torchvision.io.read_image(img_path)\n",
    "    return transform(img).unsqueeze(0).to(DEVICE)\n",
    "\n",
    "# Visualize attention patterns\n",
    "def visualize_attention(model, img):\n",
    "    with torch.no_grad():\n",
    "        features = model.feature_extractor(img)\n",
    "        transformed_features = [\n",
    "            transformer(feature.flatten(2).permute(2, 0, 1))\n",
    "            for transformer, feature in zip(model.hierarchical_transformer.transformers, features)\n",
    "        ]\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "    for i, feature in enumerate(transformed_features):\n",
    "        attention_weights = model.hierarchical_transformer.transformers[i].self_attn.compute_attention_weights(feature, feature, feature)\n",
    "        attention_map = attention_weights[0].mean(0).cpu().numpy()\n",
    "        axes[i].imshow(attention_map, cmap='viridis')\n",
    "        axes[i].set_title(f\"Scale {i+1} Attention\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Analyze model behavior on different image types\n",
    "def analyze_image_types(model, dataloader):\n",
    "    class_correct = [0] * NUM_CLASSES\n",
    "    class_total = [0] * NUM_CLASSES\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, labels in tqdm(dataloader, desc=\"Analyzing\"):\n",
    "            images, labels = images.to(DEVICE), labels.to(DEVICE)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            c = (predicted == labels).squeeze()\n",
    "            for i in range(len(labels)):\n",
    "                label = labels[i]\n",
    "                class_correct[label] += c[i].item()\n",
    "                class_total[label] += 1\n",
    "\n",
    "    classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
    "    for i in range(NUM_CLASSES):\n",
    "        print(f'Accuracy of {classes[i]}: {100 * class_correct[i] / class_total[i]:.2f}%')\n",
    "\n",
    "# Experiment with different scale configurations\n",
    "def experiment_scales(img, num_scales_list=[2, 3, 4]):\n",
    "    results = []\n",
    "    for num_scales in num_scales_list:\n",
    "        model = HierarchicalTransformer(num_scales=num_scales, d_model=256, nhead=8, num_classes=NUM_CLASSES).to(DEVICE)\n",
    "        feature_extractor = MultiScaleFeatureExtractor(num_scales=num_scales).to(DEVICE)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            features = feature_extractor(img)\n",
    "            output = model(features)\n",
    "        \n",
    "        results.append(output)\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(10, 5))\n",
    "    for i, result in enumerate(results):\n",
    "        ax.bar(np.arange(NUM_CLASSES) + i*0.25, result[0].cpu().softmax(0), width=0.25, label=f'{num_scales_list[i]} scales')\n",
    "    ax.set_xticks(np.arange(NUM_CLASSES))\n",
    "    ax.set_xticklabels(('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck'))\n",
    "    ax.legend()\n",
    "    plt.title('Model predictions with different numbers of scales')\n",
    "    plt.show()\n",
    "\n",
    "# Main execution\n",
    "if __name__ == \"__main__\":\n",
    "    model = load_model()\n",
    "    \n",
    "    # Visualize attention patterns\n",
    "    img = prepare_image('sample_image.jpg')\n",
    "    visualize_attention(model, img)\n",
    "    \n",
    "    # Analyze model behavior on different image types\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "    testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
    "    testloader = torch.utils.data.DataLoader(testset, batch_size=64, shuffle=False, num_workers=2)\n",
    "    analyze_image_types(model, testloader)\n",
    "    \n",
    "    # Experiment with different scale configurations\n",
    "    experiment_scales(img)\n",
    "\n",
    "print(\"Visualization and analysis complete.\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
