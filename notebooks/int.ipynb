{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.3.1+cu121\n",
      "0.18.1+cu121\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.models import resnet50\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from PIL import Image\n",
    "import os\n",
    "\n",
    "print(torch.__version__)\n",
    "print(torchvision.__version__)\n",
    "\n",
    "# 1. Custom Dataset for a folder of images\n",
    "class ImageFolderDataset(Dataset):\n",
    "    def __init__(self, data_dir, transform=None):\n",
    "        self.data_dir = data_dir\n",
    "        self.transform = transform\n",
    "        self.image_files = [f for f in os.listdir(data_dir) if f.lower().endswith(('.png', '.jpg', '.jpeg'))]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = os.path.join(self.data_dir, self.image_files[idx])\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, 0  # Return 0 as a dummy label\n",
    "\n",
    "# 2. Data Loading and Preprocessing\n",
    "def load_data(data_dir):\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "   \n",
    "    dataset = ImageFolderDataset(data_dir, transform=transform)\n",
    "    dataloader = DataLoader(dataset, batch_size=8, shuffle=True, num_workers=2)\n",
    "   \n",
    "    return dataloader\n",
    "\n",
    "# 3. Multi-scale Feature Extraction\n",
    "class MultiScaleFeatureExtractor(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MultiScaleFeatureExtractor, self).__init__()\n",
    "        self.resnet = resnet50(pretrained=True)\n",
    "       \n",
    "    def forward(self, x):\n",
    "        features = []\n",
    "        x = self.resnet.conv1(x)\n",
    "        x = self.resnet.bn1(x)\n",
    "        x = self.resnet.relu(x)\n",
    "        x = self.resnet.maxpool(x)\n",
    "       \n",
    "        x = self.resnet.layer1(x)\n",
    "        features.append(x)  # Fine scale\n",
    "       \n",
    "        x = self.resnet.layer2(x)\n",
    "        features.append(x)  # Medium scale\n",
    "       \n",
    "        x = self.resnet.layer3(x)\n",
    "        features.append(x)  # Coarse scale\n",
    "       \n",
    "        return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import time\n",
    "\n",
    "class ScaleSpecificTransformerEncoder(nn.Module):\n",
    "    def __init__(self, d_model, nhead, dim_feedforward=2048, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.self_attn = nn.MultiheadAttention(d_model, nhead, dropout=dropout)\n",
    "        self.linear1 = nn.Linear(d_model, dim_feedforward)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.linear2 = nn.Linear(dim_feedforward, d_model)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, src):\n",
    "        src2 = self.self_attn(src, src, src)[0]\n",
    "        src = src + self.dropout1(src2)\n",
    "        src = self.norm1(src)\n",
    "        src2 = self.linear2(self.dropout(F.relu(self.linear1(src))))\n",
    "        src = src + self.dropout2(src2)\n",
    "        src = self.norm2(src)\n",
    "        return src\n",
    "\n",
    "class CrossScaleAttention(nn.Module):\n",
    "    def __init__(self, d_model, nhead, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.multihead_attn = nn.MultiheadAttention(d_model, nhead, dropout=dropout)\n",
    "\n",
    "    def forward(self, query, key, value):\n",
    "        return self.multihead_attn(query, key, value)[0]\n",
    "\n",
    "class HierarchicalPooling(nn.Module):\n",
    "    def __init__(self, in_features, out_features):\n",
    "        super().__init__()\n",
    "        self.pool = nn.AdaptiveAvgPool2d((1, 1))  # Reduce spatial dims to 1x1\n",
    "        self.fc = nn.Linear(in_features, out_features)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x shape: [B, C, H, W]\n",
    "        x = self.pool(x)  # Shape: [B, C, 1, 1]\n",
    "        x = x.view(x.size(0), -1)  # Shape: [B, C]\n",
    "        x = self.fc(x)  # Shape: [B, out_features]\n",
    "        return x.view(x.size(0), x.size(1), 1, 1)  # Shape: [B, out_features, 1, 1]\n",
    "\n",
    "class HierarchicalTransformer(nn.Module):\n",
    "    def __init__(self, num_scales, d_model, nhead, num_classes):\n",
    "        super().__init__()\n",
    "        self.num_scales = num_scales\n",
    "        self.d_model = d_model\n",
    "\n",
    "        self.transformers = nn.ModuleList([\n",
    "            ScaleSpecificTransformerEncoder(d_model, nhead)\n",
    "            for _ in range(num_scales)\n",
    "        ])\n",
    "\n",
    "        self.cross_attentions = nn.ModuleList([\n",
    "            CrossScaleAttention(d_model, nhead)\n",
    "            for _ in range(num_scales - 1)\n",
    "        ])\n",
    "\n",
    "        self.hierarchical_pooling = nn.ModuleList([\n",
    "            HierarchicalPooling(d_model, d_model)\n",
    "            for _ in range(num_scales - 1)\n",
    "        ])\n",
    "\n",
    "        self.classifier = nn.Linear(d_model, num_classes)\n",
    "\n",
    "    def forward(self, features):\n",
    "        print(f\"HierarchicalTransformer: Input features shapes: {[f.shape for f in features]}\")\n",
    "        start_time = time.time()\n",
    "\n",
    "        # Apply scale-specific transformers\n",
    "        transformed_features = []\n",
    "        spatial_shapes = []\n",
    "        for i, (transformer, feature) in enumerate(zip(self.transformers, features)):\n",
    "            print(f\"Applying transformer for scale {i+1}\")\n",
    "            t_start = time.time()\n",
    "            B, C, H, W = feature.shape\n",
    "            spatial_shapes.append((H, W))\n",
    "            flattened = feature.flatten(2).permute(2, 0, 1)\n",
    "            print(f\"  Flattened shape: {flattened.shape}\")\n",
    "            transformed = transformer(flattened)\n",
    "            print(f\"  Transformed shape: {transformed.shape}\")\n",
    "            transformed_features.append(transformed)\n",
    "            print(f\"  Time for scale {i+1}: {time.time() - t_start:.2f} seconds\")\n",
    "\n",
    "        print(f\"Scale-specific transformers complete. Time: {time.time() - start_time:.2f} seconds\")\n",
    "\n",
    "        # Apply cross-scale attention and hierarchical pooling\n",
    "        for i in range(self.num_scales - 1, 0, -1):\n",
    "            print(f\"Processing scale {i}\")\n",
    "            t_start = time.time()\n",
    "            cross_attn = self.cross_attentions[i-1](\n",
    "                transformed_features[i-1],\n",
    "                transformed_features[i],\n",
    "                transformed_features[i]\n",
    "            )\n",
    "            print(f\"  Cross-attention shape: {cross_attn.shape}\")\n",
    "            \n",
    "            # Reshape cross_attn to 4D using the correct spatial dimensions\n",
    "            L, B, C = cross_attn.shape\n",
    "            H, W = spatial_shapes[i-1]\n",
    "            cross_attn_4d = cross_attn.permute(1, 2, 0).reshape(B, C, H, W)\n",
    "            print(f\"  Cross-attention 4D shape: {cross_attn_4d.shape}\")\n",
    "            \n",
    "            pooled = self.hierarchical_pooling[i-1](cross_attn_4d)\n",
    "            print(f\"  Pooled shape: {pooled.shape}\")\n",
    "            \n",
    "            # Reshape pooled to match transformed_features shape\n",
    "            pooled_reshaped = pooled.squeeze(-1).squeeze(-1).unsqueeze(0).expand(L, -1, -1)\n",
    "            print(f\"  Pooled reshaped: {pooled_reshaped.shape}\")\n",
    "            \n",
    "            print(f\"  Transformed feature shape before addition: {transformed_features[i-1].shape}\")\n",
    "            \n",
    "            # Ensure shapes match before addition\n",
    "            if transformed_features[i-1].shape != pooled_reshaped.shape:\n",
    "                raise ValueError(f\"Shape mismatch: {transformed_features[i-1].shape} vs {pooled_reshaped.shape}\")\n",
    "            \n",
    "            transformed_features[i-1] = transformed_features[i-1] + pooled_reshaped\n",
    "            print(f\"  Updated feature shape: {transformed_features[i-1].shape}\")\n",
    "            print(f\"  Time for scale {i}: {time.time() - t_start:.2f} seconds\")\n",
    "\n",
    "        print(f\"Cross-scale attention and pooling complete. Time: {time.time() - start_time:.2f} seconds\")\n",
    "\n",
    "        # Final classification\n",
    "        t_start = time.time()\n",
    "        output = self.classifier(transformed_features[0].mean(dim=0))\n",
    "        print(f\"Classification complete. Time: {time.time() - t_start:.2f} seconds\")\n",
    "        print(f\"Output shape: {output.shape}\")\n",
    "\n",
    "        print(f\"Total HierarchicalTransformer forward pass time: {time.time() - start_time:.2f} seconds\")\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting main execution...\n",
      "Using device: cuda\n",
      "Loading data...\n",
      "Data loaded. Time elapsed: 0.01 seconds\n",
      "Initializing feature extractor...\n",
      "Feature extractor initialized. Time elapsed: 0.45 seconds\n",
      "Initializing Hierarchical Transformer...\n",
      "Hierarchical Transformer initialized. Time elapsed: 0.48 seconds\n",
      "Processing a single batch...\n",
      "Images moved to device. Time elapsed: 0.00 seconds\n",
      "Extracting multi-scale features...\n",
      "Features extracted. Time elapsed: 0.08 seconds\n",
      "Projecting features...\n",
      "Features projected. Time elapsed: 0.51 seconds\n",
      "Processing with Hierarchical Transformer...\n",
      "HierarchicalTransformer: Input features shapes: [torch.Size([8, 128, 56, 56]), torch.Size([8, 128, 28, 28]), torch.Size([8, 128, 14, 14])]\n",
      "Applying transformer for scale 1\n",
      "  Flattened shape: torch.Size([3136, 8, 128])\n",
      "  Transformed shape: torch.Size([3136, 8, 128])\n",
      "  Time for scale 1: 0.10 seconds\n",
      "Applying transformer for scale 2\n",
      "  Flattened shape: torch.Size([784, 8, 128])\n",
      "  Transformed shape: torch.Size([784, 8, 128])\n",
      "  Time for scale 2: 0.00 seconds\n",
      "Applying transformer for scale 3\n",
      "  Flattened shape: torch.Size([196, 8, 128])\n",
      "  Transformed shape: torch.Size([196, 8, 128])\n",
      "  Time for scale 3: 0.03 seconds\n",
      "Scale-specific transformers complete. Time: 0.13 seconds\n",
      "Processing scale 2\n",
      "  Cross-attention shape: torch.Size([784, 8, 128])\n",
      "  Pooled shape: torch.Size([8, 128, 1, 1])\n",
      "  Pooled reshaped: torch.Size([1, 8, 128])\n",
      "  Transformed feature shape before addition: torch.Size([784, 8, 128])\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Shape mismatch: torch.Size([784, 8, 128]) vs torch.Size([1, 8, 128])",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 46\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFeatures projected. Time elapsed: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtime\u001b[38;5;241m.\u001b[39mtime()\u001b[38;5;250m \u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;250m \u001b[39mbatch_start_time\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m seconds\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     45\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mProcessing with Hierarchical Transformer...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 46\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mhierarchical_transformer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprojected_features\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHierarchical Transformer processing complete. Time elapsed: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtime\u001b[38;5;241m.\u001b[39mtime()\u001b[38;5;250m \u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;250m \u001b[39mbatch_start_time\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m seconds\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28;01mbreak\u001b[39;00m  \u001b[38;5;66;03m# Process only one batch\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/rapids-24.06/lib/python3.11/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/rapids-24.06/lib/python3.11/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[8], line 117\u001b[0m, in \u001b[0;36mHierarchicalTransformer.forward\u001b[0;34m(self, features)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;66;03m# Ensure shapes match before addition\u001b[39;00m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m transformed_features[i\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mshape \u001b[38;5;241m!=\u001b[39m pooled_reshaped\u001b[38;5;241m.\u001b[39mshape:\n\u001b[0;32m--> 117\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mShape mismatch: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtransformed_features[i\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m vs \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpooled_reshaped\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    119\u001b[0m transformed_features[i\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m=\u001b[39m transformed_features[i\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m+\u001b[39m pooled_reshaped\n\u001b[1;32m    120\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m  Updated feature shape: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtransformed_features[i\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mValueError\u001b[0m: Shape mismatch: torch.Size([784, 8, 128]) vs torch.Size([1, 8, 128])"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "# Main execution\n",
    "if __name__ == \"__main__\":\n",
    "    start_time = time.time()\n",
    "    \n",
    "    print(\"Starting main execution...\")\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {device}\")\n",
    "   \n",
    "    # Load data\n",
    "    print(\"Loading data...\")\n",
    "    dataloader = load_data('../test_images')\n",
    "    print(f\"Data loaded. Time elapsed: {time.time() - start_time:.2f} seconds\")\n",
    "   \n",
    "    # Initialize feature extractor\n",
    "    print(\"Initializing feature extractor...\")\n",
    "    feature_extractor = MultiScaleFeatureExtractor().to(device)\n",
    "    print(f\"Feature extractor initialized. Time elapsed: {time.time() - start_time:.2f} seconds\")\n",
    "   \n",
    "    # Initialize Hierarchical Transformer\n",
    "    print(\"Initializing Hierarchical Transformer...\")\n",
    "    num_scales = 3\n",
    "    d_model = 128  \n",
    "    nhead = 4\n",
    "    num_classes = 10\n",
    "    hierarchical_transformer = HierarchicalTransformer(num_scales, d_model, nhead, num_classes).to(device)\n",
    "    print(f\"Hierarchical Transformer initialized. Time elapsed: {time.time() - start_time:.2f} seconds\")\n",
    "   \n",
    "    # Extract features and process with Hierarchical Transformer\n",
    "    print(\"Processing a single batch...\")\n",
    "    for images, _ in dataloader:\n",
    "        batch_start_time = time.time()\n",
    "        images = images.to(device)\n",
    "        print(f\"Images moved to device. Time elapsed: {time.time() - batch_start_time:.2f} seconds\")\n",
    "        \n",
    "        print(\"Extracting multi-scale features...\")\n",
    "        multi_scale_features = feature_extractor(images)\n",
    "        print(f\"Features extracted. Time elapsed: {time.time() - batch_start_time:.2f} seconds\")\n",
    "        \n",
    "        print(\"Projecting features...\")\n",
    "        projected_features = [nn.Linear(f.size(1), d_model).to(device)(f.flatten(2).transpose(1, 2)).transpose(1, 2).reshape(f.size(0), d_model, *f.size()[2:]) for f in multi_scale_features]\n",
    "        print(f\"Features projected. Time elapsed: {time.time() - batch_start_time:.2f} seconds\")\n",
    "        \n",
    "        print(\"Processing with Hierarchical Transformer...\")\n",
    "        output = hierarchical_transformer(projected_features)\n",
    "        print(f\"Hierarchical Transformer processing complete. Time elapsed: {time.time() - batch_start_time:.2f} seconds\")\n",
    "        break  # Process only one batch\n",
    "   \n",
    "    # Visualize a sample image and its feature maps\n",
    "    print(\"Generating visualizations...\")\n",
    "    plt.figure(figsize=(20, 5))\n",
    "    plt.subplot(1, 4, 1)\n",
    "    plt.imshow(images[0].cpu().permute(1, 2, 0))\n",
    "    plt.title(\"Original Image\")\n",
    "   \n",
    "    for i, features in enumerate(multi_scale_features):\n",
    "        plt.subplot(1, 4, i+2)\n",
    "        plt.imshow(features[0].sum(dim=0).cpu().detach().numpy())\n",
    "        plt.title(f\"Scale {i+1} Features\")\n",
    "   \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('feature_visualization.png')  # Save the plot instead of showing it\n",
    "    print(\"Visualization saved as 'feature_visualization.png'\")\n",
    "\n",
    "    print(f\"Output shape from Hierarchical Transformer: {output.shape}\")\n",
    "    print(f\"Total execution time: {time.time() - start_time:.2f} seconds\")\n",
    "    print(\"Data preparation, feature extraction, and Hierarchical Transformer processing complete.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "new_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
